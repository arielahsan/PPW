{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#CRAWLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR-nSFTvnhX7",
        "outputId": "081904d5-f552-47b9-d40f-a9045e24f8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sprynger\n",
            "  Downloading sprynger-0.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sprynger) (5.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from sprynger) (2.32.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from sprynger) (2.5.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from sprynger) (4.3.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->sprynger) (2025.8.3)\n",
            "Downloading sprynger-0.4.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sprynger\n",
            "Successfully installed sprynger-0.4.1\n"
          ]
        }
      ],
      "source": [
        "pip install sprynger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDzfzZPIot2H",
        "outputId": "19857937-440a-4107-b82f-76334c6b5cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total hasil: 27\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_20\n",
            "Title: Quantifier Shifting for Quantified Boolean Formulas Revisited\n",
            "Abstract: Modern solvers for quantified Boolean formulas (QBFs) process formulas in prenex form, which divides each QBF into two parts: the quantifier prefix and the propositional matrix. While this representation does not cover the full language of QBF, every non-prenex formula can be transformed to an equivalent formula in prenex form. This transformation offers several degrees of freedom and blurs structural information that might be useful for the solvers. In a case study conducted 20 years back, it has been shown that the applied transformation strategy heavily impacts solving time. We revisit this work and investigate how sensitive recent QBF solvers perform w.r.t. various prenexing strategies.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_9\n",
            "Title: First-Order Automatic Literal Model Generation\n",
            "Abstract: Given a finite consistent set of ground literals, we present an algorithm that generates a complete first-order logic interpretation, i.e., an interpretation for all ground literals over the signature and not just those in the input set, that is also a model for the input set. The interpretation is represented by first-order linear literals. It can be effectively used to evaluate clauses. A particular application are SCL stuck states. The SCL (Simple Clause Learning) calculus always computes with respect to a finite number of ground literals. It then finds either a contradiction or a stuck state being a model with respect to the considered ground literals. Our algorithm builds a complete literal interpretation out of such a stuck state model that can then be used to evaluate the clause set. If all clauses are satisfied an overall model has been found. If it does not satisfy some clause, this information can be effectively explored to extend the scope of ground literals considered by SCL.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_3\n",
            "Title: Stepping Stones in the TPTP World\n",
            "Abstract: The TPTP World is a well established infrastructure that supports research, development, and deployment of Automated Theorem Proving (ATP) systems. There are key components that help make the TPTP World a success: the TPTP problem library was first released in 1993, the CADE ATP System Competition (CASC) was conceived after CADE-12 in 1994, problem difficulty ratings were added in 1997, the current TPTP language was adopted in 2003, the SZS ontologies were specified in 2004, the TSTP solution library was built starting around 2005, the Specialist Problem Classes (SPCs) have been used to classify problems since 2010, the SystemOnTPTP service has been offered from 2011, the StarExec service was started in 2013, and a world of TPTP users have helped all along. This paper reviews these stepping stones in the development of the TPTP World.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_21\n",
            "Title: Satisfiability Modulo Exponential Integer Arithmetic\n",
            "Abstract: SMT solvers use sophisticated techniques for polynomial (linear or non-linear) integer arithmetic. In contrast, non-polynomial integer arithmetic has mostly been neglected so far. However, in the context of program verification, polynomials are often insufficient to capture the behavior of the analyzed system without resorting to approximations. In the last years, incremental linearization has been applied successfully to satisfiability modulo real arithmetic with transcendental functions. We adapt this approach to an extension of polynomial integer arithmetic with exponential functions. Here, the key challenge is to compute suitable lemmas that eliminate the current model from the search space if it violates the semantics of exponentiation. An empirical evaluation of our implementation shows that our approach is highly effective in practice.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_6\n",
            "Title: Tableaux for Automated Reasoning in Dependently-Typed Higher-Order Logic\n",
            "Abstract: Dependent type theory gives an expressive type system facilitating succinct formalizations of mathematical concepts. In practice, it is mainly used for interactive theorem proving with intensional type theories, with PVS being a notable exception. In this paper, we present native rules for automated reasoning in a dependently-typed version (DHOL) of classical higher-order logic (HOL). DHOL has an extensional type theory with an undecidable type checking problem which contains theorem proving. We implemented the inference rules as well as an automatic type checking mode in Lash, a fork of Satallax, the leading tableaux-based prover for HOL. Our method is sound and complete with respect to provability in DHOL. Completeness is guaranteed by the incorporation of a sound and complete translation from DHOL to HOL recently proposed by Rothgang et al. While this translation can already be used as a preprocessing step to any HOL prover, to achieve better performance, our system directly works in DHOL. Moreover, experimental results show that the DHOL version of Lash can outperform all major HOL provers executed on the translation.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_22\n",
            "Title: SAT-Based Learning of Computation Tree Logic\n",
            "Abstract: The CTL learning problem consists in finding for a given sample of positive and negative Kripke structures a distinguishing CTL formula that is verified by the former but not by the latter. Further constraints may bound the size and shape of the desired formula or even ask for its minimality in terms of syntactic size. This synthesis problem is motivated by explanation generation for dissimilar models, e.g. comparing a faulty implementation with the original protocol. We devise a SAT -based encoding for a fixed size CTL formula, then provide an incremental approach that guarantees minimality. We further report on a prototype implementation whose contribution is twofold: first, it allows us to assess the efficiency of various output fragments and optimizations. Secondly, we can experimentally evaluate this tool by randomly mutating Kripke structures or syntactically introducing errors in higher-level models, then learning CTL distinguishing formulas.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_10\n",
            "Title: Synthesis of Recursive Programs in Saturation\n",
            "Abstract: We turn saturation-based theorem proving into an automated framework for recursive program synthesis. We introduce magic axioms as valid induction axioms and use them together with answer literals in saturation. We introduce new inference rules for induction in saturation and use answer literals to synthesize recursive functions from these proof steps. Our proof-of-concept implementation in the Vampire theorem prover constructs recursive functions over algebraic data types, while proving inductive properties over these types.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_23\n",
            "Title: MCSat-Based Finite Field Reasoning in the Yices2 SMT Solver (Short Paper)\n",
            "Abstract: This system description introduces an enhancement to the Yices2 SMT solver, enabling it to reason over non-linear polynomial systems over finite fields. Our reasoning approach fits into the model-constructing satisfiability (MCSat) framework and is based on zero decomposition techniques, which find finite basis explanations for theory conflicts over finite fields. As the MCSat solver within Yices2 can support (and combine) several theories via theory plugins, we implemented our reasoning approach as a new plugin for finite fields and extended Yices2 ’s frontend to parse finite field problems, making our implementation the first MCSat-based reasoning engine for finite fields. We present its evaluation on finite field benchmarks, comparing it against cvc5 . Additionally, our work leverages the modular architecture of the MCSat solver in Yices2 to provide a foundation for the rapid implementation of further reasoning techniques for this theory.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_11\n",
            "Title: Synthesizing Strongly Equivalent Logic Programs: Beth Definability for Answer Set Programs via Craig Interpolation in First-Order Logic\n",
            "Abstract: We show a projective Beth definability theorem for logic programs under the stable model semantics: For given programs P and Q and vocabulary  V (set of predicates) the existence of a program  R in V such that $$P \\cup R$$ P ∪ R and $$P \\cup Q$$ P ∪ Q are strongly equivalent can be expressed as a first-order entailment. Moreover, our result is effective: A program  R can be constructed from a Craig interpolant for this entailment, using a known first-order encoding for testing strong equivalence, which we apply in reverse to extract programs from formulas. As a further perspective, this allows transforming logic programs via transforming their first-order encodings. In a prototypical implementation, the Craig interpolation is performed by first-order provers based on clausal tableaux or resolution calculi. Our work shows how definability and interpolation, which underlie modern logic-based approaches to advanced tasks in knowledge representation, transfer to answer set programming.\n",
            "\n",
            "DOI: 10.1007/978-3-031-63498-7_16\n",
            "Title: Model Completeness for Rational Trees\n",
            "Abstract: We analyze the theory of rational trees with finitely many constructors, infinitely many atoms and an atomicity predicate. We design a new decision procedure, proving in addition that this theory is model-complete. We also show that the enrichment of the language with selectors and simultaneous parametric fixpoints enjoys quantifier elimination.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "# Silahkan membuat api key dari https://dev.springernature.com/#api\n",
        "api_key = \"bd490ef933a3b910cb1757a3e6563e03\"\n",
        "isbn = \"978-3-031-63497-0\"\n",
        "\n",
        "url = \"https://api.springernature.com/meta/v2/json\"\n",
        "params = {\n",
        "    \"q\": f\"isbn:{isbn}\",\n",
        "    \"api_key\": api_key,\n",
        "    \"p\": 10\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    print(f\"Total hasil: {data['result'][0]['total']}\\n\")\n",
        "    for record in data['records']:\n",
        "        doi = record.get('doi', 'N/A')\n",
        "        title = record.get('title', 'No title')\n",
        "        abstract = record.get('abstract', 'No abstract')\n",
        "        print(f\"DOI: {doi}\")\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"Abstract: {abstract}\\n\")\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "K6m5FLHlvvrV",
        "outputId": "4428255f-d141-4703-a4b8-80f7a31ba345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data berhasil disimpan ke springer_search_results.csv\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b4041009-0650-4879-aec5-8aca2c1771d2\", \"springer_search_results.csv\", 9299)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Silahkan membuat api key dari https://dev.springernature.com/#api\n",
        "api_key = \"bd490ef933a3b910cb1757a3e6563e03\"\n",
        "isbn = \"978-3-031-63497-0\"\n",
        "\n",
        "url = \"https://api.springernature.com/meta/v2/json\"\n",
        "params = {\n",
        "    \"q\": f\"isbn:{isbn}\",\n",
        "    \"api_key\": api_key,\n",
        "    \"p\": 10\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    records = data.get('records', [])\n",
        "\n",
        "    # Membuat list kosong untuk menyimpan data\n",
        "    data_list = []\n",
        "\n",
        "    # Iterasi melalui setiap record dan mengambil data yang relevan\n",
        "    for record in records:\n",
        "        doi = record.get('doi', 'N/A')\n",
        "        title = record.get('title', 'No title')\n",
        "        abstract = record.get('abstract', 'No abstract')\n",
        "\n",
        "        # Menambahkan data ke list\n",
        "        data_list.append({\n",
        "            'DOI': doi,\n",
        "            'Title': title,\n",
        "            'Abstract': abstract\n",
        "        })\n",
        "\n",
        "    # Membuat DataFrame dari list data\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    # Menentukan nama file CSV\n",
        "    csv_filename = \"springer_search_results.csv\"\n",
        "\n",
        "    # Menyimpan DataFrame ke file CSV\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data berhasil disimpan ke {csv_filename}\")\n",
        "\n",
        "    # Mengunduh file CSV dari Google Colab\n",
        "    files.download(csv_filename)\n",
        "\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iB68_vsIyy8g",
        "outputId": "c14b116e-c7d5-495f-963c-5405429fb642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data berhasil disimpan ke springer_search_results.csv\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c968457a-acf3-4c8c-a4bd-2333a02b0a9e\", \"springer_search_results.csv\", 18599)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Silahkan membuat api key dari https://dev.springernature.com/#api\n",
        "api_key = \"bd490ef933a3b910cb1757a3e6563e03\"\n",
        "isbn = \"978-3-031-63497-0\"\n",
        "\n",
        "url = \"https://api.springernature.com/meta/v2/json\"\n",
        "params = {\n",
        "    \"q\": f\"isbn:{isbn}\",\n",
        "    \"api_key\": api_key,\n",
        "    \"p\": 20\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    records = data.get('records', [])\n",
        "\n",
        "    # Membuat list kosong untuk menyimpan data\n",
        "    data_list = []\n",
        "\n",
        "    # Iterasi melalui setiap record dan mengambil data yang relevan\n",
        "    for record in records:\n",
        "        doi = record.get('doi', 'N/A')\n",
        "        title = record.get('title', 'No title')\n",
        "        abstract = record.get('abstract', 'No abstract')\n",
        "\n",
        "        # Menambahkan data ke list\n",
        "        data_list.append({\n",
        "            'DOI': doi,\n",
        "            'Title': title,\n",
        "            'Abstract': abstract\n",
        "        })\n",
        "\n",
        "    # Membuat DataFrame dari list data\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    # Menentukan nama file CSV\n",
        "    csv_filename = \"springer_search_results.csv\"\n",
        "\n",
        "    # Menyimpan DataFrame ke file CSV\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data berhasil disimpan ke {csv_filename}\")\n",
        "\n",
        "    # Mengunduh file CSV dari Google Colab\n",
        "    files.download(csv_filename)\n",
        "\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
